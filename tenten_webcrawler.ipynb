{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3vJrzvdAMKe"
      },
      "source": [
        "# Simple Web Crawler Implementation\n",
        "\n",
        "A simple web crawler designed here is composed of 4 main modules:\n",
        "* <b>Scheduler</b>: maintain a queue of URLs to visit\n",
        "* <b>Downloader</b>: download web pages\n",
        "* <b>Analyzer</b>: analyze content and links\n",
        "* <b>Storage</b>: store content and metadata\n",
        "\n",
        "## 1) Basic Downloader\n",
        "Every web crawler should be defined a <i>name</i> and identified its <i>owner</i> (i.e., the '`user-agent`' and '`from`' fields of the headers, respectively). Sometimes, you may get an error message, caused by the connection timeout and the page not found, for instance. You can print '`response.status_code`' to track that problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I-tNqpnWAMKp",
        "outputId": "4a27ab35-0d43-43b9-a0e2-70e6fba18e0c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests.exceptions import HTTPError\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': '6210506348',\n",
        "    'From': 'natthakit.n@ku.th'\n",
        "}\n",
        "seed_url = 'https://www.ku.ac.th/th/'\n",
        "\n",
        "def get_page(url):\n",
        "    global headers\n",
        "    text = ''\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=2)\n",
        "        # If the response was successful, no Exception will be raised\n",
        "        response.raise_for_status()\n",
        "    except HTTPError as http_err:\n",
        "        print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
        "        # return False\n",
        "    except Exception as err:\n",
        "        print(f'Other error occurred: {err}')  # Python 3.6\n",
        "        # return False\n",
        "    else:\n",
        "        print('Success!')\n",
        "        text = response.text\n",
        "    return text.lower()\n",
        "\n",
        "raw_html = get_page(seed_url)\n",
        "print(raw_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeLTIXdXAMKs"
      },
      "source": [
        "## 2) Basic Analyzer\n",
        "### 2.1 Link Parser\n",
        "The following code is an example of simple link parser. The program extracts all links by considering the <i>anchor</i> tag only, and stores them into a `urls` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5M3d05ZhAMKu",
        "outputId": "1d74b372-c6d3-418d-9ebc-bb2d8d4f5232"
      },
      "outputs": [],
      "source": [
        "def link_parser(raw_html):\n",
        "    urls = [];\n",
        "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
        "    index = 0;  length = len(raw_html)\n",
        "    while index < length:\n",
        "        start = raw_html.find(pattern_start, index)\n",
        "        if start > 0:\n",
        "            start = start + len(pattern_start)\n",
        "            end = raw_html.find(pattern_end, start)\n",
        "            link = raw_html[start:end]\n",
        "            if len(link) > 0:\n",
        "                if link not in urls:\n",
        "                    urls.append(link)\n",
        "            index = end\n",
        "        else:\n",
        "            break\n",
        "    return urls\n",
        "\n",
        "raw_html = '<html><body><a href=\"http://test1.com\">test1</a><br><a href=\"http://test2.com\">test2</a></body></html>'\n",
        "print(link_parser(raw_html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8N9OihtAMKv"
      },
      "source": [
        "### 2.2 URL Normalization\n",
        "The following code is an example of using the `urljoin()` function to transform a relative URL to the absolute one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "64VAmuGXAMKw",
        "outputId": "c7349b16-9f76-456f-fa1f-ef9619661a0e"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urljoin\n",
        "\n",
        "# Define an absolute (base) URL of a web page\n",
        "base_url = 'https://mike.cpe.ku.ac.th'\n",
        "\n",
        "# An example of the extracted absolute link\n",
        "link_1 = 'http://www.ku.ac.th'\n",
        "# An example of the extracted relative link\n",
        "link_2 = 'download/homework.html'\n",
        "\n",
        "# Resolve links\n",
        "abs_link_1 = urljoin(base_url, link_1)\n",
        "abs_link_2 = urljoin(base_url, link_2)\n",
        "\n",
        "print(abs_link_1)  # -> http://www.ku.ac.th\n",
        "print(abs_link_2)  # -> https://mike.cpe.ku.ac.th/download/homework.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Tn2JTUBwAMKx"
      },
      "source": [
        "## 3) Basic Scheduler\n",
        "The following code is an example of using a FIFO queue to handle the extracted URLs to be further downloaded. In particular, the main crawling process simply invokes the previous two defined functions, i.e., `get_page()` and `link_parser()`, to first download a web page and extract its out-going links, respectively. Then, all extracted links will be stored into a queue. We define here two queues: `frontier_q` and `visited_q`. The former is used as the FIFO queue to keep URLs for next downloading, while the latter is used to remember which web pages have been already downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0tu7u71AMKy",
        "outputId": "295ee8cf-10c6-4e94-858a-bccc4c0e5590",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "seed_url = 'https://www.ku.ac.th/th/'\n",
        "frontier_q = [seed_url]\n",
        "visited_q = []\n",
        "\n",
        "# param 'links' is a list of extracted links to be stored in the queue\n",
        "def enqueue(links):\n",
        "    global frontier_q\n",
        "    for link in links:\n",
        "        if link not in frontier_q and link not in visited_q:\n",
        "            frontier_q.append(urljoin(seed_url,link))\n",
        "\n",
        "# FIFO queue\n",
        "def dequeue():\n",
        "    global frontier_q\n",
        "    current_url = frontier_q[0]\n",
        "    frontier_q = frontier_q[1:]\n",
        "    return current_url\n",
        "\n",
        "#--- main process ---#\n",
        "current_url = dequeue()\n",
        "visited_q.append(current_url)\n",
        "raw_html = get_page(current_url)\n",
        "extracted_links = link_parser(raw_html)\n",
        "enqueue(extracted_links)\n",
        "print(frontier_q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrG7uRGxAMK0"
      },
      "source": [
        "## 4) Storing Text into a File\n",
        "As the following, we use the `os.makedirs()` function to first create (sub)directories. Notice that the `exist_ok=True` parameter is set to prevent an exception error if the target directory already exists. Then, we use the `open()`, `write()`, and `close()` functions to open a file, write some text into that file, and afterwards close it. In addition, we import the `codecs` module together with using the '`utf-8`' encoding for non-English content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0ZAJYL44AMK0",
        "outputId": "60dc3ca1-9851-4220-8542-a29add094b7a"
      },
      "outputs": [],
      "source": [
        "import os, codecs\n",
        "\n",
        "# Create (sub)directories with the 0o755 permission\n",
        "# param 'exist_ok' is True for no exception if the target directory already exists\n",
        "path = 'html/subdir1/subdir2'\n",
        "os.makedirs(path, 0o755, exist_ok=True)\n",
        "\n",
        "# Write content into a file\n",
        "raw_html = '<html><body><a href=\"http://test1.com\">test1</a><br><a href=\"http://test2.com\">test2</a></body></html>'\n",
        "raw_html = get_page('http://sis.ku.ac.th/')\n",
        "abs_file = path + '/index2' + '.html'\n",
        "f = codecs.open(abs_file, 'w', 'utf-8')\n",
        "f.write(raw_html)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6h1R55N_dq7",
        "outputId": "523a140d-0cbd-4393-b8e1-bb258bf3a62d"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "url = 'www.ku.ac.th/th/scholarships?category=120#kuyraisas'\n",
        "result = urlparse(url)\n",
        "\n",
        "print(result)\n",
        "print(result.path)\n",
        "\n",
        "filepath = 'html/' + result.netloc + result.path[:result.path.rfind('/')]\n",
        "print(filepath)\n",
        "\n",
        "filename = result.path[result.path.rfind('/')+1:] \n",
        "if result.query != '':\n",
        "  filename = filename + '?' + result.query\n",
        "if result.fragment != '':\n",
        "  filename = filename + '#' + result.fragment\n",
        "if filename == '':\n",
        "  filename = 'dummy'\n",
        "\n",
        "        \n",
        "print(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CUtsD-vAMK1"
      },
      "source": [
        "# <font color=\"blue\">Your Turn ...</font>\n",
        "Write a web crawler to collect 10,000 web pages (including only '`.htm`' and '`.html`' files) within the '`ku.ac.th`' domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWQNa8HT00JS"
      },
      "outputs": [],
      "source": [
        "seed_url = 'https://cooking.kapook.com/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gt7A6yxx-EC"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "frontier_q = ['https://nlovecooking.com','https://krua.co','https://cooking.kapook.com','https://cookpad.com/th']\n",
        "visited_q = []\n",
        "downloaded = []\n",
        "KEY_WORD = ('วัตถุดิบ','แคลอรี่','อาหาร','เมนู','ของกิน','กับข้าว','รสชาติ','อร่อย','เครื่องเคียง','ของว่าง','เครื่องดื่ม','ขนม')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J71LX3aeNXE4",
        "outputId": "ed56f4ce-0fe6-43b2-faaf-4e398699ac10"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': '6210506348',\n",
        "    'From': 'natthakit.n@ku.th'\n",
        "}\n",
        "\n",
        "\n",
        "# seed_url = 'https://www.ku.ac.th/th/'\n",
        "# seed_url = 'www.ku.ac.th/?q=th/node/add/pre-register.html'\n",
        "\n",
        "def link_parser(raw_html):\n",
        "    urls = [];\n",
        "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
        "    index = 0;  length = len(raw_html)\n",
        "    while index < length:\n",
        "        start = raw_html.find(pattern_start, index)\n",
        "        if start > 0:\n",
        "            start = start + len(pattern_start)\n",
        "            end = raw_html.find(pattern_end, start)\n",
        "            link = raw_html[start:end]\n",
        "            if len(link) > 0:\n",
        "                if link not in urls:\n",
        "                    urls.append(link)\n",
        "            index = end\n",
        "        else:\n",
        "            break\n",
        "    return urls\n",
        "\n",
        "def enqueue(links):\n",
        "    global frontier_q\n",
        "    for link in links:\n",
        "        link = urljoin(seed_url,link)\n",
        "        if link not in frontier_q and link not in visited_q:\n",
        "            frontier_q.append(link)\n",
        "\n",
        "def dequeue():\n",
        "    global frontier_q\n",
        "    current_url = frontier_q[0]\n",
        "    frontier_q = frontier_q[1:]\n",
        "    return current_url           \n",
        "\n",
        "while True:\n",
        "    current_url = dequeue()\n",
        "    if 'download' in current_url or '.pdf' in current_url:\n",
        "        continue\n",
        "\n",
        "    # print(visited_q)\n",
        "    # print(frontier_q)\n",
        "    \n",
        "    path = 'html/' + current_url.replace('https://','')\n",
        "    result = urlparse(current_url)\n",
        "    filepath = 'html/' + result.netloc + result.path[:result.path.rfind('/')]\n",
        "    filename = result.path[result.path.rfind('/')+1:] \n",
        "    \n",
        "    if result.query != '':\n",
        "        # filename = filename + '' + result.query\n",
        "        continue\n",
        "    if result.fragment != '':\n",
        "        # filename = filename + '' + result.fragment\n",
        "        continue\n",
        "\n",
        "    if filename == '':\n",
        "        filename = 'dummy'\n",
        "           \n",
        "    if len(filename) > 50 :\n",
        "        continue\n",
        "\n",
        "    if '.' in filename:\n",
        "        if '.html' not in filename or '.htm' not in filename:\n",
        "            continue\n",
        "\n",
        "    print('#',i+1)\n",
        "    visited_q.append(current_url)\n",
        "    raw_html = get_page(current_url)\n",
        "    extracted_links = link_parser(raw_html)\n",
        "\n",
        "    enqueue(extracted_links)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(filepath, 0o755, exist_ok=True)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    abs_file = filepath + '/' +  filename\n",
        "    if '.html' not in filename or '.htm' not in filename:\n",
        "        abs_file = abs_file + '.html'\n",
        "\n",
        "    try:\n",
        "        f = codecs.open(abs_file, 'w', 'utf-8')\n",
        "    except:\n",
        "        continue\n",
        "    \n",
        "    if(sum([1 for x in KEY_WORD if x in raw_html])<3):\n",
        "        continue\n",
        "\n",
        "    if 'facebook' in current_url or 'youtube' in current_url or 'google' in current_url or 'instagram' in current_url or 'twitter' in current_url:\n",
        "        continue\n",
        "\n",
        "    f.write(raw_html)\n",
        "    f.close()\n",
        "    \n",
        "    print('current_url =',current_url)\n",
        "    print('filepath =',filepath)\n",
        "    print('filename =',filename)\n",
        "    print('abs_file =',abs_file)\n",
        "    downloaded.append(current_url)\n",
        "\n",
        "    i+=1\n",
        "    if i==10000:\n",
        "        break\n",
        "\n",
        "  \n",
        "    \n",
        "    # print(frontier_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBl5EOnqy-Aa",
        "outputId": "4ef6b989-2d86-4702-8dcc-0e3cad2f8d20"
      },
      "outputs": [],
      "source": [
        "frontier_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4vexMU9X5JS"
      },
      "outputs": [],
      "source": [
        "with open('./downloaded.txt', 'w') as writefile:\n",
        "    for d in downloaded:\n",
        "        writefile.write(d+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AL-LmwCaW-w",
        "outputId": "3ea360a7-8855-4b11-d82f-60144febb379"
      },
      "outputs": [],
      "source": [
        "my_file = open(\"./downloaded.txt\", \"r\")\n",
        "content = my_file.readlines()\n",
        "for i in range(len(content)):\n",
        "    content[i] = content[i].replace('\\n','')\n",
        "len(content)\n",
        "content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwafxiwsjz7Y"
      },
      "outputs": [],
      "source": [
        "hostname = []\n",
        "for c in content:\n",
        "    url = c\n",
        "    result = urlparse(url)\n",
        "    result\n",
        "    h = result.scheme + '://' + result.netloc\n",
        "    if h not in hostname:\n",
        "        hostname.append(h)\n",
        "\n",
        "with open('./hostname.txt', 'w') as writefile:\n",
        "    for d in hostname:\n",
        "        writefile.write(d+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QQFFUwspx_O",
        "outputId": "1fbd4b6c-5833-408f-b2c1-0075b0aa9a75"
      },
      "outputs": [],
      "source": [
        "hostname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MSSL0qCm-5V",
        "outputId": "66268bce-fee8-4b61-bde0-c59cc2879b2c"
      },
      "outputs": [],
      "source": [
        "list_robot = []\n",
        "list_sitemap = []\n",
        "list_success_robot = []\n",
        "for h in hostname:\n",
        "    hb = h + '/robots.txt'\n",
        "    raw_html = get_page(hb)\n",
        "    \n",
        "    if 'user-agent' in raw_html:\n",
        "        list_robot.append(h)\n",
        "\n",
        "    if '' not in raw_html:\n",
        "        list_success_robot.append(h)\n",
        "\n",
        "    if 'sitemap' in raw_html:\n",
        "        list_sitemap.append(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKXnyZSeu6ok",
        "outputId": "e0f8dd41-eedc-4354-f9be-3ec624393c69"
      },
      "outputs": [],
      "source": [
        "list_robot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2cA9Nffuoft",
        "outputId": "03175612-641c-4df2-eb2a-cff8a3767ac7"
      },
      "outputs": [],
      "source": [
        "for lr in list_robot:\n",
        "    raw_html = get_page(lr + '/robots.txt')\n",
        "    result = urlparse(lr + '/robots.txt')\n",
        "    filepath = 'html/' + result.netloc + result.path[:result.path.rfind('/')]\n",
        "    filename = result.path[result.path.rfind('/')+1:] \n",
        "\n",
        "    abs_file = filepath + '/' + filename\n",
        "    print(abs_file)\n",
        "    os.makedirs(filepath, 0o755, exist_ok=True)\n",
        "    f = codecs.open(abs_file, 'w', 'utf-8')\n",
        "    f.write(raw_html)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKw_JRTvuDE5"
      },
      "outputs": [],
      "source": [
        "with open('./list_robots.txt', 'w') as writefile:\n",
        "    for d in list_robot:\n",
        "        writefile.write(d+'\\n')\n",
        "\n",
        "with open('./list_sitemap.txt', 'w') as writefile:\n",
        "    for d in list_sitemap:\n",
        "        writefile.write(d+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zwA9itUXr6v"
      },
      "outputs": [],
      "source": [
        "!rm -rf html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "webcrawler.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
